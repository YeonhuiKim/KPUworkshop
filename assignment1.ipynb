{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import MNISTtools\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "xtrain, ltrain=MNISTtools.load(dataset=\"training\", path=\"../mnist\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(784, 60000)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xtrain.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "47040000"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xtrain.size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "7"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "MNISTtools.show(xtrain[:,42])\n",
    "ltrain[42]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_MNIST_images(x):\n",
    "    return ((x-127.5)/127.5).astype(np.float64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0. 0. 0. 0. 0. 0. 0. 1. 0. 0.]\n",
      "7\n"
     ]
    }
   ],
   "source": [
    "def label2onehot(lbl):\n",
    "    d=np.zeros((lbl.max()+1, lbl.size))\n",
    "    d[lbl, np.arange(lbl.size)]=1\n",
    "    return d\n",
    "\n",
    "dtrain=label2onehot(ltrain)\n",
    "print(dtrain[:,42])\n",
    "print(ltrain[42])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def onehot2label(d):\n",
    "    lbl=d.argmax(axis=0)\n",
    "    return lbl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "print(all(ltrain==onehot2label(dtrain)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax(a):\n",
    "    M=a.max(axis=0)\n",
    "    z=np.exp(a-M)\n",
    "    return z/z.sum(axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmaxp(a, e):\n",
    "    return (e-np.sum(softmax(a)*e, axis=0))*softmax(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4.894580079275615e-07 should be smaller than 1e-6\n"
     ]
    }
   ],
   "source": [
    "eps=1e-6\n",
    "a=np.random.randn(10, 200)\n",
    "e=np.random.randn(10, 200)\n",
    "diff=softmaxp(a,e)\n",
    "diff_approx=(softmax(a+eps*e)-softmax(a))/eps\n",
    "rel_error=np.abs(diff-diff_approx).mean()/np.abs(diff_approx).mean()\n",
    "print(rel_error, 'should be smaller than 1e-6')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def relu(a):\n",
    "    return (a>0)*a\n",
    "    \n",
    "def relup(a, e):\n",
    "    return (a>0)*e"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_shallow(Ni, Nh, No):\n",
    "    b1 = np.random.randn(Nh, 1)   / np.sqrt((Ni+1.)/2.)\n",
    "    W1 = np.random.randn(Nh, Ni)  / np.sqrt((Ni+1.)/2.)\n",
    "    b2 = np.random.randn(No, 1)   / np.sqrt((Nh+1.))\n",
    "    W2 = np.random.randn(No, Nh)  / np.sqrt((Nh+1.))\n",
    "    return W1, b1, W2, b2\n",
    "\n",
    "Ni = xtrain.shape[0]\n",
    "Nh = 64\n",
    "No = dtrain.shape[0]\n",
    "netinit = init_shallow(Ni, Nh, No)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def forwardprop_shallow(x, net):\n",
    "    W1=net[0]\n",
    "    b1=net[1]\n",
    "    W2=net[2]\n",
    "    b2=net[3]\n",
    "    \n",
    "    a1=W1.dot(x)+b1\n",
    "    h1=relu(a1)\n",
    "    a2=W2.dot(h1)+b2\n",
    "    y=softmax(a2)\n",
    "    \n",
    "    return y\n",
    "\n",
    "xtrain=normalize_MNIST_images(xtrain)\n",
    "yinit=forwardprop_shallow(xtrain, netinit)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.2939319664114418 should be around .26\n"
     ]
    }
   ],
   "source": [
    "def eval_loss(y, d):\n",
    "    return -np.sum(d*np.log(y))/d.size\n",
    "\n",
    "print(eval_loss(yinit, dtrain), 'should be around .26')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "90.11666666666667\n"
     ]
    }
   ],
   "source": [
    "def eval_perfs(y, lbl):\n",
    "    ylabel=onehot2label(y)\n",
    "    return 100*(ylabel!=lbl).mean()\n",
    "\n",
    "print(eval_perfs(yinit, ltrain))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_shallow(x, d, net, gamma=.05):\n",
    "    W1 = net[0]\n",
    "    b1 = net[1]\n",
    "    W2 = net[2]\n",
    "    b2 = net[3]\n",
    "    Ni = W1.shape[1]\n",
    "    Nh = W1.shape[0]\n",
    "    No = W2.shape[0]\n",
    "\n",
    "    gamma = gamma / x.shape[1]\n",
    "\n",
    "    a1=W1.dot(x)+b1\n",
    "    h1=relu(a1)\n",
    "    a2=W2.dot(h1)+b2\n",
    "    y=softmax(a2)\n",
    "\n",
    "    delta2=softmaxp(a2, -d/y)\n",
    "    delta1=relup(a1, W2.T.dot(delta2))\n",
    "    \n",
    "    #경사하강법\n",
    "    W2=W2-gamma*delta2.dot(h1.T)\n",
    "    W1=W1-gamma*delta1.dot(x.T)\n",
    "    b2=b2-gamma*delta2.sum(axis=1, keepdims=True)\n",
    "    b1=b1-gamma*delta1.sum(axis=1, keepdims=True)\n",
    "    \n",
    "    return W1, b1, W2, b2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def backprop_shallow(x, d, net, T, gamma=.05):\n",
    "    lbl = onehot2label(d)\n",
    "    for t in range(T):\n",
    "        net=update_shallow(x, d, net, gamma)\n",
    "        predict=forwardprop_shallow(x, net)\n",
    "        loss=eval_loss(predict, d)\n",
    "        miss=eval_perfs(predict, lbl)\n",
    "        print(str(t)+\", loss=\"+str(loss)+\", error rate=\"+str(miss))\n",
    "    return net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0, loss=0.26942715504817566, error rate=85.80166666666666\n",
      "1, loss=0.22978220391565157, error rate=83.33\n",
      "2, loss=0.2096537928797714, error rate=71.75333333333333\n",
      "3, loss=0.20019335492197035, error rate=66.21666666666667\n",
      "4, loss=0.1931751029039113, error rate=59.458333333333336\n",
      "5, loss=0.18692273873004295, error rate=55.18333333333333\n",
      "6, loss=0.18091064677901472, error rate=51.82333333333333\n",
      "7, loss=0.17501926577580817, error rate=48.76166666666666\n",
      "8, loss=0.16919343712798635, error rate=45.82833333333333\n",
      "9, loss=0.16345891618157543, error rate=43.415\n",
      "10, loss=0.157904927720347, error rate=41.193333333333335\n",
      "11, loss=0.15262640923228682, error rate=39.19833333333334\n",
      "12, loss=0.14760507481640955, error rate=37.62333333333333\n",
      "13, loss=0.14280733144648872, error rate=36.06333333333333\n",
      "14, loss=0.13822400474687982, error rate=34.64\n",
      "15, loss=0.13385209686718075, error rate=33.29666666666667\n",
      "16, loss=0.12968810668298336, error rate=32.016666666666666\n",
      "17, loss=0.12573145234185606, error rate=30.925000000000004\n",
      "18, loss=0.12197732087407276, error rate=29.831666666666667\n",
      "19, loss=0.11841865388205547, error rate=28.89\n",
      "20, loss=0.11504877718672145, error rate=27.908333333333335\n",
      "21, loss=0.11185972466497672, error rate=27.075\n",
      "22, loss=0.10884393811646165, error rate=26.275\n",
      "23, loss=0.10599965008865513, error rate=25.498333333333335\n",
      "24, loss=0.10331622188248961, error rate=24.765\n",
      "25, loss=0.10078602790461577, error rate=24.201666666666664\n",
      "26, loss=0.09840296551695411, error rate=23.598333333333333\n",
      "27, loss=0.09615954550029301, error rate=23.081666666666667\n",
      "28, loss=0.09404856915866419, error rate=22.538333333333334\n",
      "29, loss=0.09206376403567108, error rate=22.16\n",
      "30, loss=0.09019441282570141, error rate=21.736666666666668\n",
      "31, loss=0.08844247411256309, error rate=21.47833333333333\n",
      "32, loss=0.08679773156562312, error rate=21.058333333333334\n",
      "33, loss=0.08527600312636432, error rate=20.903333333333332\n",
      "34, loss=0.08385044610597157, error rate=20.553333333333335\n",
      "35, loss=0.08257108156827402, error rate=20.463333333333335\n",
      "36, loss=0.08136634795554643, error rate=20.233333333333334\n",
      "37, loss=0.08036252491140962, error rate=20.331666666666667\n",
      "38, loss=0.0793691694895686, error rate=20.233333333333334\n",
      "39, loss=0.07867978265651891, error rate=20.486666666666668\n",
      "40, loss=0.07783184589502908, error rate=20.43166666666667\n",
      "41, loss=0.07744757823223707, error rate=20.825\n",
      "42, loss=0.07657424587229945, error rate=20.673333333333332\n",
      "43, loss=0.07635818756165347, error rate=21.118333333333332\n",
      "44, loss=0.07524217246811496, error rate=20.798333333333332\n",
      "45, loss=0.07496737888190197, error rate=21.071666666666665\n",
      "46, loss=0.07353996850022913, error rate=20.503333333333334\n",
      "47, loss=0.07304792343993291, error rate=20.5\n",
      "48, loss=0.0714861141955403, error rate=19.841666666666665\n",
      "49, loss=0.07080439572117611, error rate=19.63\n",
      "50, loss=0.06933137460580072, error rate=19.040000000000003\n",
      "51, loss=0.06857321989127585, error rate=18.781666666666666\n",
      "52, loss=0.06729917478802244, error rate=18.240000000000002\n",
      "53, loss=0.06654951911231323, error rate=18.016666666666666\n",
      "54, loss=0.06548363263296128, error rate=17.551666666666666\n",
      "55, loss=0.06478102669816438, error rate=17.34\n",
      "56, loss=0.06388796383629283, error rate=16.99\n",
      "57, loss=0.06324110683213938, error rate=16.798333333333336\n",
      "58, loss=0.06248074178760134, error rate=16.583333333333332\n",
      "59, loss=0.06188727413431898, error rate=16.386666666666667\n",
      "60, loss=0.0612244724474666, error rate=16.233333333333334\n",
      "61, loss=0.06067823008334468, error rate=15.968333333333334\n",
      "62, loss=0.06009088708951577, error rate=15.871666666666668\n",
      "63, loss=0.059585738374026344, error rate=15.646666666666667\n",
      "64, loss=0.05905658014552412, error rate=15.553333333333333\n",
      "65, loss=0.05858662155521083, error rate=15.443333333333333\n",
      "66, loss=0.058103897891763165, error rate=15.318333333333333\n",
      "67, loss=0.05766444495995716, error rate=15.233333333333333\n",
      "68, loss=0.05721908791678257, error rate=15.086666666666668\n",
      "69, loss=0.05680724645473531, error rate=15.016666666666667\n",
      "70, loss=0.05639325540115119, error rate=14.905\n",
      "71, loss=0.0560053990990908, error rate=14.816666666666666\n",
      "72, loss=0.05561747524023866, error rate=14.715\n",
      "73, loss=0.05525037544095479, error rate=14.655000000000001\n",
      "74, loss=0.05488627781880431, error rate=14.585\n",
      "75, loss=0.054539072371973474, error rate=14.488333333333333\n",
      "76, loss=0.05419608851512347, error rate=14.416666666666666\n",
      "77, loss=0.05386659948823791, error rate=14.336666666666668\n",
      "78, loss=0.05354231869310662, error rate=14.261666666666667\n",
      "79, loss=0.05322919679920135, error rate=14.178333333333335\n",
      "80, loss=0.05292181044504034, error rate=14.153333333333334\n",
      "81, loss=0.05262399158981437, error rate=14.061666666666667\n",
      "82, loss=0.052331484185922324, error rate=13.988333333333333\n",
      "83, loss=0.052047656245337, error rate=13.918333333333333\n",
      "84, loss=0.05176914118784525, error rate=13.858333333333334\n",
      "85, loss=0.05149811759658812, error rate=13.763333333333334\n",
      "86, loss=0.05123240829573846, error rate=13.741666666666665\n",
      "87, loss=0.05097328397658437, error rate=13.641666666666666\n",
      "88, loss=0.05071929047682175, error rate=13.613333333333333\n",
      "89, loss=0.05047131484547797, error rate=13.571666666666667\n",
      "90, loss=0.050228468402480596, error rate=13.518333333333333\n",
      "91, loss=0.04999112152156339, error rate=13.466666666666665\n",
      "92, loss=0.049758496514080944, error rate=13.406666666666666\n",
      "93, loss=0.049530912304901475, error rate=13.358333333333333\n",
      "94, loss=0.049307781576936056, error rate=13.283333333333333\n",
      "95, loss=0.049089300336353836, error rate=13.255\n",
      "96, loss=0.04887508639898967, error rate=13.200000000000001\n",
      "97, loss=0.04866531907757489, error rate=13.18\n",
      "98, loss=0.048459652631010576, error rate=13.114999999999998\n",
      "99, loss=0.04825795163363599, error rate=13.091666666666665\n"
     ]
    }
   ],
   "source": [
    "nettrain = backprop_shallow(xtrain, dtrain, netinit, 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(784, 10000)\n",
      "(10000,)\n"
     ]
    }
   ],
   "source": [
    "xtest, ltest = MNISTtools.load(dataset = \"testing\", path = \"../mnist\")\n",
    "xtest = normalize_MNIST_images(xtest)\n",
    "dtest = label2onehot(ltest)\n",
    "print(xtest.shape)\n",
    "print(ltest.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.22983651407383482\n",
      "83.28999999999999\n"
     ]
    }
   ],
   "source": [
    "predict = forwardprop_shallow(xtest,nettrain)\n",
    "loss = eval_loss(predict,dtest)\n",
    "miss = eval_perfs(predict,ltest)\n",
    "print(loss)\n",
    "print(miss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.03066241133439734 9.396666666666667\n",
      "0.02334410650064302 7.12\n",
      "0.019318530361918388 5.908333333333333\n",
      "0.01660705048598481 5.101666666666667\n",
      "0.014883119948518922 4.585\n"
     ]
    }
   ],
   "source": [
    "def backprop_minibatch_shallow(x, d, net, T, B=100, gamma=.05):\n",
    "    N=x.shape[1]\n",
    "    NB=int((N+B-1)/B)\n",
    "    lbl=onehot2label(d)\n",
    "    for t in range(T):\n",
    "        shuffled_indices=np.random.permutation(range(N))\n",
    "        for l in range(NB):\n",
    "            minibatch_indices=shuffled_indices[B*l:min(B*(l+1), N)]\n",
    "            index=np.arange(B*l, min(B*(l+1), N))\n",
    "            net=update_shallow(x[:, index], d[:, index], net, gamma)\n",
    "        y=forwardprop_shallow(x, net)\n",
    "        loss=eval_loss(y, d)\n",
    "        miss=eval_perfs(y, lbl)\n",
    "        print(str(loss)+\" \"+str(miss))\n",
    "\n",
    "    return net\n",
    "\n",
    "netminibatch=backprop_minibatch_shallow(xtrain, dtrain, netinit, 5, B=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.015396330490438079\n",
      "4.8500000000000005\n"
     ]
    }
   ],
   "source": [
    "prd=forwardprop_shallow(xtest, netminibatch)\n",
    "loss = eval_loss(prd,dtest)\n",
    "miss = eval_perfs(prd,ltest)\n",
    "print(loss)\n",
    "print(miss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
